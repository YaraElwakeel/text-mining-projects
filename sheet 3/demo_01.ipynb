{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import pandas as pd\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0                     Hi.                 Salut!\n",
       "1                    Run!                Cours !\n",
       "2                    Run!               Courez !\n",
       "3                    Who?                  Qui ?\n",
       "4                    Wow!             Ça alors !"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"eng_-french.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputEmbedding(\n",
      "  (embedding): Embedding(10, 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class InputEmbedding (nn.Module) :\n",
    "    def __init__(self,d_model:int, vocab_size:int):\n",
    "        super(InputEmbedding,self).__init__()\n",
    "\n",
    "        self.d_model= d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding= nn.Embedding(vocab_size,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # this multiplication helps maintain the appropriate variance of the input embeddings.\n",
    "        return self.embedding(x)*math.sqrt(self.d_model)\n",
    "\n",
    "model = InputEmbedding(10,10)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional encoding block "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for even positions \n",
    "$$\n",
    "PE(pos,2i) = sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n",
    "$$\n",
    "for odd positions\n",
    "$$\n",
    "PE(pos,2i+1) = cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n",
    "$$\n",
    "\n",
    "however we are calculating the divisor in the log scale for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionalEncoding(\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model:int, seq_len:int,dropout:float):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "\n",
    "        self.d_model = d_model \n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # create a matrix of seq_len * d_model \n",
    "        Pos_enc = torch.zeros(seq_len,d_model)\n",
    "        # create a vector of shape (seq_len)\n",
    "        position = torch.arange(0,seq_len,dtype = torch.float).unsqueeze(1) # (seq_len,1)\n",
    "        # compute the divisor\n",
    "        div_term = torch.exp(torch.arange(0,d_model, 2)).float()* (-math.log(10000.0)/d_model)\n",
    "\n",
    "        Pos_enc [:,0::2] = torch.sin(position * div_term)\n",
    "        Pos_enc [:,1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # add an additional dimension for the batch size \n",
    "        Pos_enc = Pos_enc.unsqueeze(1) # [1,seq_len,d_model]\n",
    "        self.register_buffer(\"Pos_enc\",Pos_enc) # as this in unlearnable parameter, to save it with the model \n",
    "\n",
    "\n",
    "        def forward(self,x):\n",
    "            # add every pos_enc to every embedding of the word to account for placement of the word in a sentence\n",
    "            # make it an unlearnable parameter as it's fixed \n",
    "            x = x + self.Pos_enc [:,x.shape(1),:].requires_grad(False)\n",
    "            x = self.dropout(x)\n",
    "            return x \n",
    "    \n",
    "model= PositionalEncoding(10,10,0.55)\n",
    "print(model)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add and Norm block \n",
    "These blocks incorporate two essential components: a residual connection and a LayerNormalization layer.\n",
    "\n",
    "### Position-Wise Feed-Forward Network (FFN):\n",
    "FFN consists of two fully connected layers. Number of dimensions in the hidden layer $d_{ff}$, is generally set to around four times that of the token embedding $d_{model}$. So it is sometimes also called the expand-and-contract network.\n",
    "There is an activation at the hidden layer, which is usually set to ReLU activation\n",
    "\n",
    "The FFN transforms the features of each position in the input sequence independently.\n",
    "By processing each position separately, the FFN enables the model to capture position-specific information and learn different representations for different parts of the sequence.\n",
    "\n",
    "The **expanding** action increases the dimensionality of the representations, allowing the model to capture more complex features and interactions in the data, while the **contracting** action compresses these representations, preserving the most relevant information and reducing computational complexity, thereby improving the model's efficiency and capacity to capture intricate patterns.\n",
    "\n",
    "$$FFN(x,W_1,W_2,b_1,b_2)=max(0,xW_1+b_1)W_2+b_2$$\n",
    "where $W_1, W_2, b_1$ and $b_2$ are learnable parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (normlayer): LayerNorm((1, 102, 10), eps=1e-05, elementwise_affine=True)\n",
      "  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,embedd_shape,d_model, dropout):\n",
    "        super(Encoder,self).__init__()\n",
    "\n",
    "        self.embedd_shape = embedd_shape\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        # norm block \n",
    "        self.normlayer = nn.LayerNorm(self.embedd_shape)\n",
    "\n",
    "        #FFN block \n",
    "        d_ff = d_model *4\n",
    "        self.linear1 = nn.Linear(self.d_model,d_ff) # W_1 and b_1\n",
    "        self.linear2 = nn.Linear(d_ff, self.d_model) # W_2 and b_2\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x = self.normlayer(x)\n",
    "        x = self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "        return x \n",
    "    \n",
    "model = Encoder([1,102,10],512,0.5)\n",
    "# model = Encoder()\n",
    "print(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi head attention block  \n",
    "\n",
    "attention mechanisms were introduced to give access to all sequence elements at each time step. The key is to be selective and determine which words are most important in a specific context.\n",
    "\n",
    "Self-attention is a mechanism used in deep learning models, that enhances the information content of an input embedding by incorporating information about the input's context. It allows the model to assign different weights to different words in a sequence, focusing more on relevant parts and less on irrelevant ones, thus enriching the representation of the input sequence. \n",
    "\n",
    "Each word in a sequence is transformed into three vectors: Query (Q), Key (K), and Value (V) by multiplying the word's embedding by learnable weights. This process is done to capture various information about the word for Q, K, and V, which are then fed into our attention layer:\n",
    "- Q-> The query vector that represents the word for which we want to calculate the attention scores. It's the vector that we will compare with other words in the sequence to determine their relevance to the current word.\n",
    "\n",
    "- K -> The key vector represents the other words in the sequence. Each word has its own key vector. These key vectors are compared with the query vector to determine how relevant each word is to the query word.\n",
    "- V -> The value vector that carries information about the word itself. After determining the relevance of each word (using keys and queries), these values are combined to create the output. \n",
    "\n",
    "Computing the dot product of the Query vector of one word with the Key vector of another word, divided by the square root of the dimensionality of the vectors, produces a score that represents the importance of the relationship between the two words, which is then passed through a softmax function to get attention weights, and finally, these attention weights are used to compute a weighted sum of the Value vectors, providing the context vector.\n",
    "Finally, the model uses this weighted sum to create a new representation for each word that takes into account its relationship with all the other words in the sentence. This representation captures the context in which the word appears\n",
    "$$\n",
    "Attention(Q,K,V) = softmax (\\frac{Qk^T}{\\sqrt{d_{model}}}) V\n",
    "$$\n",
    "Multi-head allows the model to focus on different aspects of the input simultaneously, improving its ability to capture complex relationships within the sequence.\n",
    "1.\tSplitting into Heads: In multi-head self-attention, the input is transformed into multiple smaller representations, called \"heads\". Each head has its own set of learned weight matrices for query (Q), key (K), and value (V) transformations. These weight matrices are learned during training.\n",
    "2.\tParallel Computations: Each head performs its own attention calculation independently, resulting in multiple sets of attention scores.\n",
    "3.\tConcatenation and Linear Transformation: After the attention scores are calculated for each head, they are concatenated together and multiplied by a learned weight matrix. This linear transformation ensures that the outputs from different heads are combined appropriately.\n",
    "$$\n",
    "MultiHead(Q, K, V ) = Concat(head_1, ..., head_h) W_O\n",
    "$$\n",
    "$$\n",
    "\\quad  \\textrm{where} \\quad  head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
