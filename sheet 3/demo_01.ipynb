{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import pandas as pd\n",
    "import math \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0                     Hi.                 Salut!\n",
       "1                    Run!                Cours !\n",
       "2                    Run!               Courez !\n",
       "3                    Who?                  Qui ?\n",
       "4                    Wow!             Ça alors !"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"eng_-french.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputEmbedding(\n",
      "  (embedding): Embedding(10, 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class InputEmbedding (nn.Module) :\n",
    "    def __init__(self,d_model:int, vocab_size:int):\n",
    "        super(InputEmbedding,self).__init__()\n",
    "\n",
    "        self.d_model= d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding= nn.Embedding(vocab_size,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # this multiplication helps maintain the appropriate variance of the input embeddings.\n",
    "        return self.embedding(x)*math.sqrt(self.d_model)\n",
    "\n",
    "model = InputEmbedding(10,10)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding block "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for even positions \n",
    "$$\n",
    "PE(pos,2i) = sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n",
    "$$\n",
    "for odd positions\n",
    "$$\n",
    "PE(pos,2i+1) = cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n",
    "$$\n",
    "\n",
    "however we are calculating the divisor in the log scale for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionalEncoding(\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model:int, seq_len:int,dropout:float):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "\n",
    "        self.d_model = d_model \n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # create a matrix of seq_len * d_model \n",
    "        Pos_enc = torch.zeros(seq_len,d_model)\n",
    "        # create a vector of shape (seq_len)\n",
    "        position = torch.arange(0,seq_len,dtype = torch.float).unsqueeze(1) # (seq_len,1)\n",
    "        # compute the divisor\n",
    "        div_term = torch.exp(torch.arange(0,d_model, 2)).float()* (-math.log(10000.0)/d_model)\n",
    "\n",
    "        Pos_enc [:,0::2] = torch.sin(position * div_term)\n",
    "        Pos_enc [:,1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # add an additional dimension for the batch size \n",
    "        Pos_enc = Pos_enc.unsqueeze(1) # [1,seq_len,d_model]\n",
    "        self.register_buffer(\"Pos_enc\",Pos_enc) # as this in unlearnable parameter, to save it with the model \n",
    "\n",
    "\n",
    "        def forward(self,x):\n",
    "            # add every pos_enc to every embedding of the word to account for placement of the word in a sentence\n",
    "            # make it an unlearnable parameter as it's fixed \n",
    "            x = x + self.Pos_enc [:,x.shape(1),:].requires_grad(False)\n",
    "            x = self.dropout(x)\n",
    "            return x \n",
    "    \n",
    "model= PositionalEncoding(10,10,0.55)\n",
    "print(model)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add and Norm block \n",
    "These blocks incorporate two essential components: a residual connection and a LayerNormalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm (nn.Module):\n",
    "\n",
    "    def __init__(self, d_model,dropout:float):\n",
    "        super(AddNorm,self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # norm block \n",
    "        self.normlayer = nn.LayerNorm(d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, sublayer):\n",
    "        x = x + self.dropout(self.normlayer(sublayer(x)))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-Wise Feed-Forward Network (FFN):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFN consists of two fully connected layers. Number of dimensions in the hidden layer $d_{ff}$, is generally set to around four times that of the token embedding $d_{model}$. So it is sometimes also called the expand-and-contract network.\n",
    "There is an activation at the hidden layer, which is usually set to ReLU activation\n",
    "\n",
    "The FFN transforms the features of each position in the input sequence independently.\n",
    "By processing each position separately, the FFN enables the model to capture position-specific information and learn different representations for different parts of the sequence.\n",
    "\n",
    "The **expanding** action increases the dimensionality of the representations, allowing the model to capture more complex features and interactions in the data, while the **contracting** action compresses these representations, preserving the most relevant information and reducing computational complexity, thereby improving the model's efficiency and capacity to capture intricate patterns.\n",
    "\n",
    "$$FFN(x,W_1,W_2,b_1,b_2)=max(0,xW_1+b_1)W_2+b_2$$\n",
    "where $W_1, W_2, b_1$ and $b_2$ are learnable parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self,d_model, dropout):\n",
    "        super(FFN,self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        #FFN block \n",
    "        d_ff = d_model *4\n",
    "        self.linear1 = nn.Linear(self.d_model,d_ff) # W_1 and b_1\n",
    "        self.linear2 = nn.Linear(d_ff, self.d_model) # W_2 and b_2\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "        return x \n",
    "    \n",
    "# model = FFN([1,102,10],512,0.5)\n",
    "# model = Encoder()\n",
    "# print(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead self-attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention mechanisms were introduced to give access to all sequence elements at each time step. The key is to be selective and determine which words are most important in a specific context.\n",
    "\n",
    "Self-attention is a mechanism used in deep learning models, that enhances the information content of an input embedding by incorporating information about the input's context. It allows the model to assign different weights to different words in a sequence, focusing more on relevant parts and less on irrelevant ones, thus enriching the representation of the input sequence. \n",
    "\n",
    "Each word in a sequence is transformed into three vectors: Query (Q), Key (K), and Value (V) by multiplying the word's embedding by learnable weights. This process is done to capture various information about the word for Q, K, and V, which are then fed into our attention layer:\n",
    "- Q-> The query vector that represents the word for which we want to calculate the attention scores. It's the vector that we will compare with other words in the sequence to determine their relevance to the current word.\n",
    "\n",
    "- K -> The key vector represents the other words in the sequence. Each word has its own key vector. These key vectors are compared with the query vector to determine how relevant each word is to the query word.\n",
    "- V -> The value vector that carries information about the word itself. After determining the relevance of each word (using keys and queries), these values are combined to create the output. \n",
    "\n",
    "Computing the dot product of the Query vector of one word with the Key vector of another word, divided by the square root of the dimensionality of the vectors, produces a score that represents the importance of the relationship between the two words, which is then passed through a softmax function to get attention weights, and finally, these attention weights are used to compute a weighted sum of the Value vectors, providing the context vector.\n",
    "Finally, the model uses this weighted sum to create a new representation for each word that takes into account its relationship with all the other words in the sentence. This representation captures the context in which the word appears\n",
    "$$\n",
    "Attention(Q,K,V) = softmax (\\frac{Qk^T}{\\sqrt{d_{model}}}) V\n",
    "$$\n",
    "Multi-head allows the model to focus on different aspects of the input simultaneously, improving its ability to capture complex relationships within the sequence.\n",
    "1.\tSplitting into Heads: In multi-head self-attention, the input is transformed into multiple smaller representations, called \"heads\". Each head has its own set of learned weight matrices for query (Q), key (K), and value (V) transformations. These weight matrices are learned during training.\n",
    "2.\tParallel Computations: Each head performs its own attention calculation independently, resulting in multiple sets of attention scores.\n",
    "3.\tConcatenation and Linear Transformation: After the attention scores are calculated for each head, they are concatenated together and multiplied by a learned weight matrix. This linear transformation ensures that the outputs from different heads are combined appropriately.\n",
    "$$\n",
    "MultiHead(Q, K, V ) = Concat(head_1, ..., head_h) W_O\n",
    "$$\n",
    "$$\n",
    "\\quad  \\textrm{where} \\quad  head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h: int , d_model:int, dropout:float):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "\n",
    "        self.h = h \n",
    "        self.d_model = d_model\n",
    " \n",
    "        # check is it's possible to divide d_model amongst the available heads h \n",
    "        assert d_model  % h == 0, \"d_model is not divisible by h\"\n",
    "        #split d_model into the multitude of heads \n",
    "        d_k = d_model// h\n",
    "\n",
    "        # linear transformation matrices\n",
    "        self.w_q = nn.Linear(d_model,d_model)\n",
    "        self.w_k = nn.Linear(d_model,d_model) \n",
    "        self.w_v = nn.Linear(d_model,d_model) \n",
    "        self.w_o = nn.Linear(d_model,d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(Q,K,V,mask,dropout: nn.Dropout):\n",
    "\n",
    "        d_k = V.shape[-1]\n",
    "        #[batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_values = (Q @ K.Transpose(-2,-1))/ math.sqrt(d_k)\n",
    "\n",
    "        # MASKED SELF ATTENTION THAT IS USED IN THE DECODER \n",
    "        # if mask is not None : \n",
    "            # attention_value.fill_mask(mask = 0, -1e9)\n",
    "\n",
    "        # dim=-1 sp that the softmax function normalizes the scores for each query across all keys\n",
    "        attention_values = attention_values.softmax(dim = -1) \n",
    "\n",
    "        if dropout : \n",
    "            attention_values = dropout(attention_values)\n",
    "\n",
    "        attention_f_values = attention_values @ V\n",
    "\n",
    "        return  attention_f_values, attention_values\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,Q,K,V,mask = None):\n",
    "\n",
    "        # linear transformation \n",
    "        # -> (batch_size, seq_length , d_model)\n",
    "        Q = self.w_q(Q) \n",
    "        K = self.w_k(K)\n",
    "        V = self.w_v(V)\n",
    "\n",
    "        # splitting by viewing each matrix as a (batch_size, seq_length, h , d_k)\n",
    "        # change the shape to (batch__size, num_heads, seq_length, d_k)\n",
    "        Q = Q.view(Q.shape[0],Q.shape[1], self.h , self.d_k).transpose(1,2)\n",
    "        K = K.view(K.shape[0],K.shape[1], self.h , self.d_k).transpose(1,2)\n",
    "        V = V.view(V.shape[0],V.shape[1], self.h , self.d_k).transpose(1,2)\n",
    "\n",
    "        x, self.attention_values = MultiHeadAttention.attention(Q,K,V,mask,self.dropout)\n",
    "\n",
    "        # return the shape to (batch_size, seq_length,num_head,s d_k) \n",
    "        #Concatenate the results of all the heads. (batch_size, seq_len, d_model)\n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0],-1,self.h*self.d_k)\n",
    "\n",
    "        x = self.w_o(x)\n",
    "\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder wrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,SelfAttention_block :MultiHeadAttention, FFN_block :FFN ,dropout:float) :\n",
    "        super(EncoderBlock,self).__init__()\n",
    "\n",
    "        self.SelfAttention_block = SelfAttention_block \n",
    "        self.FFN_block = FFN_block\n",
    "        #ModuleList for storing and iterating over a list of modules.\n",
    "        self.AddNorm_block = nn.ModuleList([AddNorm(dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self ,x , mask):\n",
    "        x = self.AddNorm_block[0](x,self.SelfAttention_block(x,x,x,mask))\n",
    "        x = self.AddNorm_block[1](x,self.FFN_block(x))\n",
    "\n",
    "        return x \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers :nn.ModuleList,d_model):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = layers \n",
    "        self.normlayer = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return self.normlayer(x) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock (nn.Module):\n",
    "    def __init__(self,MaskedSelfAtt: MultiHeadAttention,CrossAttention:MultiHeadAttention, FNN_block : FFN, dropout:float):\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.MaskedSelfAtt = MaskedSelfAtt\n",
    "        self.CrossAttention = CrossAttention\n",
    "        self.FNN_block = FNN_block \n",
    "        self.AddNorm = nn.ModuleList([AddNorm(dropout) for _ in range[3]])\n",
    "\n",
    "        \n",
    "    def forward(self, x , encoder_output , encoder_mask , decoder_mask ):\n",
    "        # 2ftkry shofy  leeh hena mstkhdm lambda \n",
    "        # x = self.AddNorm[0](x, lambda x : self.MaskedSelfAtt(x,x,x,deco_mask))\n",
    "        x = self.AddNorm[0](x, self.MaskedSelfAtt(x,x,x,decoder_mask))\n",
    "        x = self.AddNorm[1](x, self.CrossAttention(x,encoder_output,encoder_output,encoder_mask))\n",
    "        # leeeh msh wakhod 2y input hena fl FNN \n",
    "        x = self.AddNorm[2](x, self.FNN_block)\n",
    "        return x \n",
    "    \n",
    "class Decoder (nn.Module):\n",
    "    def __init__(self,layers:nn.ModuleList,d_model):\n",
    "        super(Decoder,self).__init__()\n",
    "\n",
    "        self.layers = layers \n",
    "        self.normlayer = nn.LayerNorm(d_model)\n",
    "    def forward (self,x,encoder_output,encoder_mask, decoder_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,encoder_output,encoder_mask, decoder_mask)\n",
    "        return self.normlayer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead (nn.Module):\n",
    "    def __init__(self,d_model,vocab_size):\n",
    "        super(ClassificationHead,self).__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward (self,x):\n",
    "        logits = self.linear(x)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer wrapper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " during inference, you can reuse the output of the encoder in a Transformer model. This is a common practice, especially in sequence-to-sequence tasks like machine translation, where the encoder processes the input sequence once and the decoder generates the output sequence token by token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder:Encoder, decoder:Decoder, input_embedding:InputEmbedding , output_embedding:InputEmbedding, \n",
    "                 input_pos : PositionalEncoding,output_pos:PositionalEncoding, classification_head:ClassificationHead) :\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder \n",
    "        self.input_embedding = input_embedding\n",
    "        self.output_embedding = output_embedding\n",
    "        self.input_pos = input_pos\n",
    "        self.output_pos = output_pos\n",
    "        self.classification_head = classification_head\n",
    "\n",
    "    def forward(self):\n",
    "        \n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
