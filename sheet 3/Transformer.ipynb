{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import pandas as pd\n",
    "import math \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding (nn.Module) :\n",
    "    def __init__(self,d_model:int, vocab_size:int):\n",
    "        super(InputEmbedding,self).__init__()\n",
    "\n",
    "        self.d_model= d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding= nn.Embedding(vocab_size,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # this multiplication helps maintain the appropriate variance of the input embeddings.\n",
    "        return self.embedding(x)*math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding block "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for even positions \n",
    "$$\n",
    "PE(pos,2i) = sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n",
    "$$\n",
    "for odd positions\n",
    "$$\n",
    "PE(pos,2i+1) = cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n",
    "$$\n",
    "\n",
    "however we are calculating the divisor in the log scale for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a matrix of seq_len * d_model\n",
    "        Pos_enc = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len, 1)\n",
    "        # Compute the divisor\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        Pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        Pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add an additional dimension for the batch size\n",
    "        Pos_enc = Pos_enc.unsqueeze(0)  # [1, seq_len, d_model]\n",
    "        # as this in unlearnable parameter save it with the model as a buffer\n",
    "        self.register_buffer(\"Pos_enc\", Pos_enc)  # Register as a buffer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to each embedding in the batch\n",
    "        # make it an unlearnable parameter as it's fixed\n",
    "        x = x + self.Pos_enc[:, :x.size(1), :].requires_grad_(False)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add and Norm block \n",
    "These blocks incorporate two essential components: a residual connection and a LayerNormalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm (nn.Module):\n",
    "\n",
    "    def __init__(self, d_model,dropout:float):\n",
    "        super(AddNorm,self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # norm block \n",
    "        self.normlayer = nn.LayerNorm(d_model)\n",
    "        \n",
    "\n",
    "    def forward(self,x, sublayer):\n",
    "        x = x + self.dropout(self.normlayer(sublayer))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-Wise Feed-Forward Network (FFN):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFN consists of two fully connected layers. Number of dimensions in the hidden layer $d_{ff}$, is generally set to around four times that of the token embedding $d_{model}$. So it is sometimes also called the expand-and-contract network.\n",
    "There is an activation at the hidden layer, which is usually set to ReLU activation\n",
    "\n",
    "The FFN transforms the features of each position in the input sequence independently.\n",
    "By processing each position separately, the FFN enables the model to capture position-specific information and learn different representations for different parts of the sequence.\n",
    "\n",
    "The **expanding** action increases the dimensionality of the representations, allowing the model to capture more complex features and interactions in the data, while the **contracting** action compresses these representations, preserving the most relevant information and reducing computational complexity, thereby improving the model's efficiency and capacity to capture intricate patterns.\n",
    "\n",
    "$$FFN(x,W_1,W_2,b_1,b_2)=max(0,xW_1+b_1)W_2+b_2$$\n",
    "where $W_1, W_2, b_1$ and $b_2$ are learnable parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self,d_model, dropout):\n",
    "        super(FFN,self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        #FFN block \n",
    "        d_ff = d_model *4\n",
    "        self.linear1 = nn.Linear(self.d_model,d_ff) # W_1 and b_1\n",
    "        self.linear2 = nn.Linear(d_ff, self.d_model) # W_2 and b_2\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "        return x \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead self-attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention mechanisms were introduced to give access to all sequence elements at each time step. The key is to be selective and determine which words are most important in a specific context.\n",
    "\n",
    "Self-attention is a mechanism used in deep learning models, that enhances the information content of an input embedding by incorporating information about the input's context. It allows the model to assign different weights to different words in a sequence, focusing more on relevant parts and less on irrelevant ones, thus enriching the representation of the input sequence. \n",
    "\n",
    "Each word in a sequence is transformed into three vectors: Query (Q), Key (K), and Value (V) by multiplying the word's embedding by learnable weights. This process is done to capture various information about the word for Q, K, and V, which are then fed into our attention layer:\n",
    "- Q-> The query vector that represents the word for which we want to calculate the attention scores. It's the vector that we will compare with other words in the sequence to determine their relevance to the current word.\n",
    "\n",
    "- K -> The key vector represents the other words in the sequence. Each word has its own key vector. These key vectors are compared with the query vector to determine how relevant each word is to the query word.\n",
    "- V -> The value vector that carries information about the word itself. After determining the relevance of each word (using keys and queries), these values are combined to create the output. \n",
    "\n",
    "Computing the dot product of the Query vector of one word with the Key vector of another word, divided by the square root of the dimensionality of the vectors, produces a score that represents the importance of the relationship between the two words, which is then passed through a softmax function to get attention weights, and finally, these attention weights are used to compute a weighted sum of the Value vectors, providing the context vector.\n",
    "Finally, the model uses this weighted sum to create a new representation for each word that takes into account its relationship with all the other words in the sentence. This representation captures the context in which the word appears\n",
    "$$\n",
    "Attention(Q,K,V) = softmax (\\frac{Qk^T}{\\sqrt{d_{model}}}) V\n",
    "$$\n",
    "Multi-head allows the model to focus on different aspects of the input simultaneously, improving its ability to capture complex relationships within the sequence.\n",
    "1.\tSplitting into Heads: In multi-head self-attention, the input is transformed into multiple smaller representations, called \"heads\". Each head has its own set of learned weight matrices for query (Q), key (K), and value (V) transformations. These weight matrices are learned during training.\n",
    "2.\tParallel Computations: Each head performs its own attention calculation independently, resulting in multiple sets of attention scores.\n",
    "3.\tConcatenation and Linear Transformation: After the attention scores are calculated for each head, they are concatenated together and multiplied by a learned weight matrix. This linear transformation ensures that the outputs from different heads are combined appropriately.\n",
    "$$\n",
    "MultiHead(Q, K, V ) = Concat(head_1, ..., head_h) W_O\n",
    "$$\n",
    "$$\n",
    "\\quad  \\textrm{where} \\quad  head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h: int , d_model:int, dropout:float):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "\n",
    "        self.h = h \n",
    "        self.d_model = d_model\n",
    " \n",
    "        # check is it's possible to divide d_model amongst the available heads h \n",
    "        assert d_model  % h == 0, \"d_model is not divisible by h\"\n",
    "        #split d_model into the multitude of heads \n",
    "        self.d_k = d_model// h\n",
    "\n",
    "        # linear transformation matrices\n",
    "        self.w_q = nn.Linear(d_model,d_model)\n",
    "        self.w_k = nn.Linear(d_model,d_model) \n",
    "        self.w_v = nn.Linear(d_model,d_model) \n",
    "        self.w_o = nn.Linear(d_model,d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(Q,K,V,mask,dropout: nn.Dropout):\n",
    "\n",
    "        d_k = V.shape[-1]\n",
    "        #[batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_values = (Q @ K.transpose(-2,-1))/ math.sqrt(d_k)\n",
    "        # MASKED SELF ATTENTION \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # ensure mask shape (batch_size, 1, seq_len)\n",
    "            attention_values = attention_values.masked_fill(mask == 0, float('-inf'))\n",
    "        # dim=-1 so that the softmax function normalizes the scores for each query across all keys\n",
    "        attention_values = attention_values.softmax(dim = -1) \n",
    "        attention_values = dropout(attention_values)\n",
    "        attention_f_values = attention_values @ V\n",
    "\n",
    "        return  attention_f_values, attention_values\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,Q,K,V,mask = None):\n",
    "\n",
    "        # linear transformation \n",
    "        # -> (batch_size, seq_length , d_model)\n",
    "        Q = self.w_q(Q) \n",
    "        K = self.w_k(K)\n",
    "        V = self.w_v(V)\n",
    "\n",
    "        # splitting by viewing each matrix as a (batch_size, seq_length, h , d_k)\n",
    "        # change the shape to (batch__size, num_heads, seq_length, d_k)\n",
    "        Q = Q.view(Q.shape[0],Q.shape[1], self.h , self.d_k).transpose(1,2)\n",
    "        K = K.view(K.shape[0],K.shape[1], self.h , self.d_k).transpose(1,2)\n",
    "        V = V.view(V.shape[0],V.shape[1], self.h , self.d_k).transpose(1,2)\n",
    "\n",
    "        x, self.attention_values = MultiHeadAttention.attention(Q,K,V,mask,self.dropout)\n",
    "\n",
    "        # return the shape to (batch_size, seq_length,num_head,s d_k) \n",
    "        #Concatenate the results of all the heads. (batch_size, seq_len, d_model)\n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0],-1,self.h*self.d_k)\n",
    "\n",
    "        x = self.w_o(x)\n",
    "\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder wrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,SelfAttention_block :MultiHeadAttention, FFN_block :FFN ,dropout:float,d_model) :\n",
    "        super(EncoderBlock,self).__init__()\n",
    "\n",
    "        self.SelfAttention_block = SelfAttention_block \n",
    "        self.FFN_block = FFN_block\n",
    "        #ModuleList for storing and iterating over a list of modules.\n",
    "        self.AddNorm_block = nn.ModuleList([AddNorm(d_model,dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self ,x , mask):\n",
    "        x = self.AddNorm_block[0](x,self.SelfAttention_block(x,x,x,mask))\n",
    "        x = self.AddNorm_block[1](x,self.FFN_block(x))\n",
    "\n",
    "        return x \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers :nn.ModuleList,d_model):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = layers \n",
    "        self.normlayer = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return self.normlayer(x) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock (nn.Module):\n",
    "    def __init__(self,MaskedSelfAtt: MultiHeadAttention,CrossAttention:MultiHeadAttention, FNN_block : FFN, dropout:float,d_model):\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.MaskedSelfAtt = MaskedSelfAtt\n",
    "        self.CrossAttention = CrossAttention\n",
    "        self.FNN_block = FNN_block \n",
    "        self.AddNorm = nn.ModuleList([AddNorm(d_model,dropout) for _ in range(3)])\n",
    "\n",
    "        \n",
    "    def forward(self, x , encoder_output , encoder_mask , decoder_mask ):\n",
    "        x = self.AddNorm[0](x, self.MaskedSelfAtt(x,x,x,decoder_mask))\n",
    "        x = self.AddNorm[1](x, self.CrossAttention(x,encoder_output,encoder_output,encoder_mask))\n",
    "        # leeeh msh wakhod 2y input hena fl FNN \n",
    "        \n",
    "        x = self.AddNorm[2](x, self.FNN_block(x))\n",
    "        return x \n",
    "    \n",
    "class Decoder (nn.Module):\n",
    "    def __init__(self,layers:nn.ModuleList,d_model):\n",
    "        super(Decoder,self).__init__()\n",
    "\n",
    "        self.layers = layers \n",
    "        self.normlayer = nn.LayerNorm(d_model)\n",
    "    def forward (self,x,encoder_output,encoder_mask, decoder_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,encoder_output,encoder_mask, decoder_mask)\n",
    "        return self.normlayer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead (nn.Module):\n",
    "    def __init__(self,d_model,vocab_size):\n",
    "        super(ClassificationHead,self).__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward (self,x):\n",
    "        logits = self.linear(x)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer wrapper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " during inference, you can reuse the output of the encoder in a Transformer model. This is a common practice, especially in sequence-to-sequence tasks like machine translation, where the encoder processes the input sequence once and the decoder generates the output sequence token by token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,src_vocab_size:int,tgt_vocab_size:int,src_seq_len:int,tgt_seq_len:int,dropout:float,\n",
    "                 d_model: int = 512, N:int = 6,h:int = 8 ):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.src_seq_len = src_seq_len\n",
    "        self.tgt_seq_len = tgt_seq_len\n",
    "        self.N = N\n",
    "        self.h = h \n",
    "        self.dropout = dropout\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        # embedding layer \n",
    "        self.src_embedd = InputEmbedding(self.d_model,self.src_vocab_size)\n",
    "        self.tgt_embedd = InputEmbedding(self.d_model,self.tgt_vocab_size)\n",
    "\n",
    "        # positional encoding layer \n",
    "        self.src_pos = PositionalEncoding(self.d_model,self.src_seq_len,self.dropout)\n",
    "        self.tgt_pos = PositionalEncoding(self.d_model,self.tgt_seq_len,self.dropout)\n",
    "\n",
    "        # Encoders block \n",
    "        encoder_blocks = []\n",
    "        for _ in range(self.N):\n",
    "            encoder_self_attention = MultiHeadAttention(self.h,self.d_model,self.dropout)\n",
    "            FFN_layer = FFN(self.d_model,self.dropout)\n",
    "            encoder_block = EncoderBlock(encoder_self_attention,FFN_layer,self.dropout,self.d_model)\n",
    "            encoder_blocks.append(encoder_block)\n",
    "        \n",
    "        #decoders block \n",
    "        decoder_blocks = []\n",
    "        for _ in range(self.N):\n",
    "            decoder_self_attention = MultiHeadAttention(self.h, self.d_model, self.dropout) \n",
    "            decoder_cross_attention = MultiHeadAttention(self.h, self.d_model, self.dropout) \n",
    "            FFN_layer = FFN(self.d_model,self.dropout)\n",
    "            decoder_block = DecoderBlock(decoder_self_attention,decoder_cross_attention,FFN_layer,self.dropout,self.d_model)\n",
    "            decoder_blocks.append(decoder_block)\n",
    "\n",
    "        # create encoder and decoder \n",
    "        self.encoder = Encoder (encoder_blocks,self.d_model)\n",
    "        self.decoder = Decoder (decoder_blocks,self.d_model)\n",
    "\n",
    "        # classification head \n",
    "        self.classification_head = ClassificationHead(self.d_model,self.tgt_vocab_size)\n",
    "\n",
    "    def forward(self,src,tgt , tgt_mask=None,src_mask=None):\n",
    "        src_embedd = self.src_embedd(src)\n",
    "        src_pos =  self.src_pos(src_embedd)\n",
    "        encoder_output = self.encoder(src_pos, src_mask)\n",
    "\n",
    "        tgt_embedd = self.tgt_embedd(tgt)\n",
    "        tgt_pos =  self.tgt_pos(tgt_embedd)\n",
    "        decoder_output = self.decoder(tgt_pos,encoder_output,src_mask, tgt_mask)\n",
    "\n",
    "        classify = self.classification_head(decoder_output)\n",
    "\n",
    "        return classify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.1472e-04, 1.7049e-04, 1.1314e-04,  ..., 6.1232e-05,\n",
      "          1.2523e-04, 7.9625e-05],\n",
      "         [1.1360e-05, 8.4765e-05, 4.0626e-05,  ..., 1.7509e-04,\n",
      "          6.2689e-05, 2.7224e-04],\n",
      "         [2.8805e-04, 1.3874e-04, 1.6954e-04,  ..., 1.2933e-04,\n",
      "          1.8462e-04, 1.8869e-04],\n",
      "         ...,\n",
      "         [7.2598e-05, 9.9030e-05, 1.4380e-04,  ..., 3.4544e-05,\n",
      "          6.2701e-05, 2.0583e-04],\n",
      "         [2.1606e-04, 2.2514e-05, 1.1037e-04,  ..., 1.6708e-04,\n",
      "          5.8799e-05, 1.0975e-04],\n",
      "         [2.8416e-05, 2.0974e-04, 6.6932e-05,  ..., 9.1073e-05,\n",
      "          1.4830e-04, 3.2595e-05]],\n",
      "\n",
      "        [[4.7407e-05, 4.5760e-05, 1.1797e-04,  ..., 6.1123e-05,\n",
      "          9.2908e-05, 4.9942e-05],\n",
      "         [1.4749e-04, 1.0644e-04, 3.4683e-04,  ..., 8.0093e-05,\n",
      "          6.9632e-05, 6.1360e-05],\n",
      "         [7.7821e-05, 2.5662e-05, 1.2113e-04,  ..., 4.0259e-05,\n",
      "          5.7442e-05, 7.6457e-05],\n",
      "         ...,\n",
      "         [5.6182e-05, 6.1656e-05, 1.0672e-04,  ..., 1.7616e-04,\n",
      "          7.2058e-05, 4.6187e-05],\n",
      "         [8.1953e-05, 5.5676e-05, 1.7903e-04,  ..., 1.7876e-04,\n",
      "          5.8348e-05, 1.3297e-04],\n",
      "         [9.2984e-05, 2.0627e-04, 3.5243e-05,  ..., 1.5844e-04,\n",
      "          3.4492e-05, 1.0243e-04]],\n",
      "\n",
      "        [[7.1170e-05, 6.6832e-05, 1.4637e-04,  ..., 9.0347e-05,\n",
      "          6.4105e-05, 7.0269e-05],\n",
      "         [1.1599e-04, 1.8670e-05, 6.5436e-05,  ..., 8.8163e-05,\n",
      "          1.8081e-04, 6.0787e-05],\n",
      "         [7.6990e-05, 6.3856e-05, 9.1952e-05,  ..., 2.3481e-04,\n",
      "          1.7381e-04, 1.3089e-04],\n",
      "         ...,\n",
      "         [5.6638e-05, 7.7384e-05, 6.9805e-05,  ..., 1.5525e-04,\n",
      "          9.2045e-05, 1.2368e-04],\n",
      "         [3.4347e-05, 4.2588e-05, 1.5109e-04,  ..., 1.0029e-04,\n",
      "          1.2011e-04, 2.0855e-04],\n",
      "         [8.9954e-05, 6.4172e-05, 9.0568e-05,  ..., 3.4737e-04,\n",
      "          3.7432e-05, 8.6282e-05]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.2459e-04, 1.2589e-04, 1.0614e-04,  ..., 6.9607e-05,\n",
      "          1.6908e-04, 4.9879e-05],\n",
      "         [5.2612e-05, 8.5859e-05, 9.8808e-05,  ..., 1.7665e-04,\n",
      "          4.0546e-05, 6.7865e-05],\n",
      "         [1.6738e-04, 1.1572e-04, 1.9210e-04,  ..., 9.5345e-05,\n",
      "          5.2507e-05, 1.5949e-04],\n",
      "         ...,\n",
      "         [9.4897e-05, 4.4824e-05, 4.7820e-05,  ..., 8.4349e-05,\n",
      "          7.2949e-05, 5.4982e-05],\n",
      "         [1.5018e-04, 1.8043e-05, 2.0850e-04,  ..., 4.3300e-05,\n",
      "          8.1562e-05, 4.8206e-05],\n",
      "         [3.9816e-05, 1.4805e-04, 8.4734e-05,  ..., 6.1807e-05,\n",
      "          5.8238e-05, 3.0947e-05]],\n",
      "\n",
      "        [[1.1638e-04, 7.3374e-05, 7.0131e-05,  ..., 9.8343e-05,\n",
      "          5.8138e-05, 1.6974e-04],\n",
      "         [1.4199e-04, 1.6258e-04, 1.0623e-04,  ..., 1.0426e-04,\n",
      "          4.1760e-05, 2.5641e-04],\n",
      "         [3.5920e-05, 6.0211e-05, 1.3183e-04,  ..., 5.7305e-05,\n",
      "          8.7186e-05, 9.6476e-05],\n",
      "         ...,\n",
      "         [2.4587e-04, 1.5802e-04, 2.7090e-05,  ..., 2.4799e-04,\n",
      "          7.2348e-05, 1.3201e-04],\n",
      "         [4.1076e-05, 1.0988e-04, 1.4562e-04,  ..., 1.1037e-04,\n",
      "          8.9710e-05, 1.3861e-04],\n",
      "         [1.6601e-04, 2.6524e-04, 1.3265e-04,  ..., 6.0433e-05,\n",
      "          1.4518e-04, 4.7283e-05]],\n",
      "\n",
      "        [[1.1362e-04, 1.7522e-04, 7.1719e-05,  ..., 8.2875e-05,\n",
      "          4.5663e-05, 7.4067e-05],\n",
      "         [8.7712e-05, 7.9542e-05, 2.0944e-04,  ..., 4.3441e-05,\n",
      "          1.0039e-04, 6.9809e-05],\n",
      "         [1.0135e-04, 1.3225e-04, 2.4860e-05,  ..., 9.3303e-05,\n",
      "          8.3697e-05, 6.4255e-05],\n",
      "         ...,\n",
      "         [1.7184e-04, 8.7753e-05, 1.7403e-04,  ..., 3.0476e-04,\n",
      "          1.1697e-04, 3.6782e-05],\n",
      "         [7.5847e-05, 4.2134e-05, 2.3379e-05,  ..., 1.2310e-04,\n",
      "          4.5765e-05, 1.2987e-04],\n",
      "         [1.3434e-04, 1.0200e-04, 6.3060e-05,  ..., 8.7857e-05,\n",
      "          2.4153e-04, 4.1266e-05]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "src_vocab_size = 10000\n",
    "tgt_vocab_size = 10000\n",
    "src_seq_len = 100\n",
    "tgt_seq_len = 100\n",
    "dropout = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "src = torch.randint(0, src_vocab_size, (batch_size, src_seq_len))  \n",
    "tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_len))\n",
    "\n",
    "# Create masks\n",
    "def create_masks(src, tgt, src_pad_idx, tgt_pad_idx):\n",
    "    src_mask = (src != src_pad_idx).unsqueeze(-2)\n",
    "    tgt_mask = (tgt != tgt_pad_idx).unsqueeze(-2)\n",
    "    \n",
    "    # Subsequent mask for the target sequence (for autoregressive decoding)\n",
    "    size = tgt.size(1)  # get seq_len for matrix\n",
    "    nopeak_mask = torch.tril(torch.ones((1, size, size), device=tgt.device)).bool()\n",
    "    tgt_mask = tgt_mask & nopeak_mask\n",
    "    \n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "src_pad_idx = 0  # Assume 0 is the padding index\n",
    "tgt_pad_idx = 0  # Assume 0 is the padding index\n",
    "\n",
    "# src_mask prevents attention to source padding tokens.\n",
    "# tgt_mask enforces autoregressive decoding by masking future target tokens.\n",
    "src_mask, tgt_mask = create_masks(src, tgt, src_pad_idx, tgt_pad_idx)\n",
    "\n",
    "\n",
    "model = Transformer( src_vocab_size, tgt_vocab_size, \n",
    "                     src_seq_len, tgt_seq_len, dropout)\n",
    "x = model(src,tgt,src_mask,tgt_mask)  \n",
    "print(x)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"eng_-french.csv\")\n",
    "english_sentences = df['English words/sentences'].tolist()\n",
    "french_sentences = df['French words/sentences'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Autoencoders: Explain what autoencoders are and how they are related to sequence-to-sequence modeling. How are they related to Transformers?**\n",
    "> ##### Autoencoders: \n",
    "an auto encoder is a type of neural nets designed to learn efficient representations of the input data that can be used for either for dimensionality reduction or feature learning. it's structured as follows :\n",
    "\n",
    "- **input layer**: that receives an input of size d\n",
    "- encoder: that encodes the input data into a latent representation of size k << d , which is usually composed of one or more layers of neurons with activation functions \n",
    "- **latent/ bottleneck layer**: the latent representation we obtain from the encoder , which forces the model to learn a compressed version of our input, the dimensionality reduction at this stage ensures that the model only learns the relevant most important features of the input.\n",
    "- **decoder**: which reconstructs the input x from the latent representation either to reproduce a reconstructed version original input x or  apply them to a supervised task. \n",
    "\n",
    "this model can be used for a lot of purposes for instance , Dimensionality Reduction to compress the input into a much lower dimensions and reconstructing it afterwords for storage or visualization purposes, feature learning to learn abstract features in an unsupervised manner that can be used in a supervised tasks and sequence to sequence(seq2seq) tasks.\n",
    "\n",
    "> ##### Encoder-Decoder seq2seq models: \n",
    "when it comes to seq2seq tasks such as language translation or text summarization, using a standard neural network in an encoder is unsufficient due to the sequential nature of the data. instead RNN or LSTM are used. where autoencoders are adapted to to accommodate LSTMs instead of neural net. additionally, it solved the challenges that accompanied LSTM or other RNN variants which is the variable length input and output  : \n",
    "- **RNN/LSTM sequence encoder**: encoder process each token in the input sequence and tries to represent it using a fixed length latent layer( also known as context vector in the context of seq2seq) and after going through all the tokens the encoder passes this newly learned vector onto the decoder \n",
    "- **RNN/LSTM sequence decoder** : takes the context vector from the encoder and generates the output sequences. it produces on element at a time, updating it's hidden states based on the previously generated elements, allowing for flexible mappings between sequences of different lengths. \n",
    "\n",
    "> ##### Transformers \n",
    "the main disadvantage of Encoder-decoder seq2seq models is their fixed length context vector design thus with long sentences it struggles to learn long term dependency due to the vanishing/exploding gradient dilemma, it only remembers the parts that it has just learned , another problem is that there is no way to attend to words more than others for their importance ,that's where Transformers came in handy, it's an encoder-decoder structure however it relies solely on self-attention mechanisms to weigh the importance of different words in a sequence. allowing each position in the input sequence to attend to all other positions, enabling the model to capture dependencies regardless of distance. where structurly:\n",
    "\n",
    "- encoder: Consists of multiple layers of self-attention and feedforward neural networks. also, the input sequence is processed all at once, allowing for parallelization.\n",
    "- decoder: Similar to the encoder but includes an additional attention mechanism to focus on relevant parts of the input sequence when generating each output element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.Normalization: What are the differences between the notions of batch and layer normalization? Explain your answer** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization techniques can significantly reduce the training time of a model by normalizing each feature, ensuring the network remains unbiased towards higher value features. Batch normalization and layer normalization are two popular methods used for this purpose. and here are some of the key differences between them:\n",
    "\n",
    ">##### Batch Normalization (BatchNorm) \n",
    "- normalizes the activation functions across a mini-batch of a definite size,at which it normalizes the activation for each mini-batch of training data. It calculates the mean and variance across the mini-batch dimension for each feature.\n",
    "- \n",
    "$$\n",
    "\\hat{x_i} = \\frac{x_i -\\mu_B }{\\sqrt{\\sigma^2_B + \\epsilon}}\n",
    "$$\n",
    "$\\mu_B$: batch mean <br>\n",
    "$\\sigma_B^2$: batch variance <br>\n",
    "$\\epsilon$: is a small constant for numerical stability \n",
    "- Adds noise during training, which acts as a form of regularization and reduces the probability of overfitting.\n",
    "- Computationally more costly because it calculates the mean and variance for the activations of each mini-batch.\n",
    "- Typically used in convolutional neural networks (CNNs) and feedforward neural networks (FFNs) where batch processing is common.\n",
    ">##### Layer Normalization (LayerNorm)\n",
    "- Normalizes the activations across all dimensions (features) of the input for each example independently, instead of across the batch dimension.\n",
    "- \n",
    "$$\n",
    "\\hat{x_i} = \\frac{x_i -\\mu_L }{\\sqrt{\\sigma^2_L + \\epsilon}}\n",
    "$$\n",
    "$\\mu_B$: mean along a layer <br>\n",
    "$\\sigma_B^2$: variance along a layer<br> \n",
    "$\\epsilon$: is a small constant for numerical stability\n",
    "- Does not inherently add noise during training, so it does not have the same regularization effect as batch normalization.\n",
    "-Less computationally expensive as it calculates the mean and variance for each example individually.\n",
    "- Mostly used in recurrent neural networks (RNNs) and transformer-based architectures, where input lengths can vary and batch sizes may be small or varied. \n",
    "\n",
    "After normalization using either of the two techniques , a learned affine transformation is applied to the normalized features. \n",
    "$$ \n",
    "y = \\gamma\\hat{x_i}+\\beta\n",
    "​$$\n",
    "where $\\gamma$ and $\\beta$ are learnable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Show how training and inference of RNNs and Transformers for sequence-to-sequence modeling are done in an autoregressive fashion. What trick do we use when training the decoder part? Explain your answer in detail.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Recurrent Neural Networks (RNNs) and Transformers are used for sequence-to-sequence (seq2seq) tasks. These models generate output sequences one element at a time in an autoregressive fashion, where autoregressive modeling means that the model predicts the next element in the sequence based on the elements that have been generated so far. This approach is used in both training and inference phases, though the techniques differ slightly to ensure effective learning and generation.\n",
    "\n",
    "> **RNN in seq2seq**: \n",
    "\n",
    "In RNNs, the input sequence is given sequentially. Each element of the input sequence is fed into the RNN's encoder one at a time, and the encoder updates its hidden state with each new element, capturing the temporal dependencies in the sequence. \n",
    "\n",
    "> **transformers in seq2seq**\n",
    "\n",
    "However transformers's , unlike RNNs, process the input sequence in parallel rather than sequentially, where this is achieved due to the self attention mechanism, which allows the model to attend to all positions in the input sequence simultaneously, capturing complex dependencies more efficiently.\n",
    "\n",
    "\n",
    ">**inference in seq2seq modeling**  \n",
    "\n",
    "for inference, both algorithms uses their own previously generated token at the previous time stamp as input for the next step where the autoregressive continues until end of sequence token is generated This method ensures that the model can generate coherent and contextually appropriate sequences based on the elements generated so far.\n",
    "\n",
    ">**Training in Seq2Seq Models with Teacher Forcing**\n",
    "\n",
    "for training ,the trick used here to ensure effective learning instead of using the model's own output , the model's true output are used rather than the predicted one ,which is a technique called \"teacher forcing\" ,as if we refed the our model the previous time step it will lead to slow convergence and model instability during the training phase.The loss is computed between the predicted output sequence and the actual target sequence, allowing the model to adjust its parameters to improve its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Positional embeddings: Explain in your own words what positional embeddings are and why they are introduced? What alternative methods (other than the sin-cos based method we discussed in the lecture) are usually considered for incorporating positional embeddings? Explain your results verbally as well as formally.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Attention mechanism: What are the main differences between the attention models considered in RNNs and the Transformer-models? What is the main motivation to incorporate attention to RNNs? Why do we consider using multi-headed attention in Transformers? What is modeled through it? Explain your answer in detail.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
