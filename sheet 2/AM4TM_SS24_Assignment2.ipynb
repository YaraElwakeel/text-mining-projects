{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission - Exercises sheet 2\n",
    "MA-INF 4236 - Advanced Methods for Text Mining, So24<br>\n",
    "Yara Elwakeel 50135730<br>\n",
    "Aksa Aksa 50146305<br>\n",
    "Sadia Naseer 50194450<br>\n",
    "Ali Ather 50194294<br>\n",
    "Affan Zafar 50167759<br>\n",
    "june 19, 2024<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) \n",
    "### 5.1)\n",
    "#### MLP \n",
    "A Multi-Layer Perceptron (MLP) consists of an input layer, one or more hidden layers, and an output layer. The connections between these layers are feedforward and unidirectional, meaning each neuron in a layer is connected to every neuron in the subsequent layer. This fully connected nature allows MLPs to effectively capture complex patterns in the data. <br>\n",
    "MLPs are best suited for static datasets, where the data points are independent and identically distributed (i.i.d.), such as image classification, regression tasks, and other problems where the order of the data does not matter. Because MLPs do not have any inherent memory of previous inputs, they treat each input individually without considering any temporal dependencies.\n",
    "#### RNN \n",
    "A Recurrent Neural Network (RNN) can be considered an extension of an MLP with additional features that enable it to handle sequential data. The key feature that distinguishes RNNs from MLPs is the presence of feedback connections. These connections allow the network to transmit information from neurons in later layers back to neurons in earlier layers. This introduces a notion of time delay, ensuring that information from one time step \n",
    "ùë° is carried over to the next time step t+1 without alteration. This mechanism enables the network to maintain a memory of past states and capture sequential dependencies over time.<br>\n",
    "RNNs are particularly effective for tasks that involve sequential data where the order and timing of the inputs matter. This includes applications such as language modeling, where the meaning of a word can depend on the previous words in the sentence, and time-series prediction\n",
    "<br>\n",
    "<br>\n",
    "an example of vanilla RNN is elman network, at which the system contains a system state(memory / context neuron) $h_t$ to model dependencies at a given time t:\n",
    "$$\n",
    "h_t = f^h (Bh_{t-1}+ Ax_t)\n",
    "$$\n",
    "$$\n",
    "x_{t+1} = f^0 (Ch_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "long term dependency is a problem most vanilla RNN methods suffers from, which refers to the difficulty they face when learning and capturing dependencies that span over a considerable number of time steps, as by each info passing by each time step, the gradient can either explode or vanish during training. this phenomena makes it challenging for the network to effectively learn and retain info over long dependencies.\n",
    "LSTM and GRU are explicitly designed to counter this problem where the main idea is control information flow through gating mechanisms\n",
    "- LSTMs: Use multiple gates to control the cell state explicitly, allowing selective forgetting, updating, and outputting of information. This fine-grained control helps in maintaining long-term dependencies.\n",
    "\n",
    "- GRUs: Use reset and update gates to merge the hidden state and memory state, simplifying the process while still effectively managing the flow of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structures \n",
    "##### LSTM \n",
    "LSTMs divide the context management problem into two subproblems: removing information no longer\n",
    "needed from the context, and adding information likely to be needed for later decision making.<br>\n",
    "the key for LSTM success is cell state which maintains information across time steps thus preserving long term dependencies (long term memory) and gates that regulates the flow of information. which are 3 in total, these gates share a common design pattern; each consists of a feedforward layer, followed by a sigmoid activation function, followed by a pointwise\n",
    "multiplication with the layer being gated:\n",
    "- forget gate:\n",
    "<br>\n",
    "The purpose of this gate is\n",
    "to delete information from the cell state that is no longer needed.(fraction of the previous cell states to forget)\n",
    "$$\n",
    "f_t = \\sigma (Ax_t + Bh_{t-1}+a) \n",
    "$$\n",
    "- input gate <br>\n",
    "This is done in two stages, first is to compute the actual information we need to extract from the previous hidden state and current inputs, a candidate value that we might want to save into our cell state.\n",
    "$$\n",
    "\\hat{c_t} = tanh(Cx_t + Dh_{t-1}+ b)\n",
    "$$\n",
    "to control the amount of fraction of new learned information same as before (percentage of potential memory to remember)\n",
    "$$\n",
    "i_T = \\sigma (Ex_t + Fh_{t-1}+c) \n",
    "$$\n",
    "then update the cell state, by multiplying the old state by $f_t$, forgetting the things that we decided to forget and add the new fraction of learned information \n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\hat{c_t}\n",
    "$$\n",
    "- output gate: \n",
    "<br>\n",
    "The final gate we‚Äôll use, which is used to decide what information is required for the current hidden state \n",
    "$$\n",
    "o_t = \\sigma (Gx_t + Hh_{t-1}+d) \n",
    "$$\n",
    "$$\n",
    "h_t = o_t \\odot tanh(c_t)\n",
    "$$\n",
    "where $h_t$ is the hidden state, that provides the output for the LSTM at each time step.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU \n",
    "a GRU is a simplified version of LSTM , as instead of three gates(input, output and forget) now GRU has only two gates. Additionally,GRU combines the cell state and hidden state.: \n",
    "- update gate :\n",
    "which determines how much of the past information (previous hidden state $h_{t-1}$)  should be carried forward along the future(current hidden state $h_t$)\n",
    "<br>\n",
    "$$\n",
    "z_t = \\sigma(Ax_t +Bh_{t‚àí1} +a)\n",
    "$$\n",
    "where $\\sigma $ is the sigmoid activation function , A,B are weight matrices and a is the bias term .\n",
    "- Reset Gate: \n",
    "<br>\n",
    "determines how much of the previous hidden state ($h_{t-1}$) should be discarded. \n",
    "$$\n",
    "r_t = \\sigma(Cx_t +Dh_{t‚àí1} +b)\n",
    "$$\n",
    "<br>\n",
    "all that is left is to compute the candidate hidden state , incorporating the reset gate to control how much of the previous hidden state influence the candidate state .\n",
    "\n",
    "$$ \n",
    "\\hat{h_t} = tanh (Ex_t + F(r_t \\odot h_{t-1}) + c)\n",
    "$$\n",
    "\n",
    "afterwords, combine the previous hidden state and the candidate hidden state using the update gate that determines the proportion of the candidate activation and the previous hidden state to use. when $ z_t $ is close to 1 , more of the candidate state is used and vise versa. \n",
    "$$\n",
    "h_t = z_t \\odot \\hat{h_t} + (1- z_t) \\odot h_{t-1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main differences between LSTM and GRU : \n",
    "- LSTM has an extra gate and thus extra parameters than GRU , which gives it more flexibility and expressiveness but more computational cost and risk of overfitting. on the contrary, GRU possess fewer gates and parameters which makes it faster and simpler but less powerful and adaptable \n",
    "\n",
    "- LSTM has separate cell state and hidden state, which allows it to store and output different views of the input , while GRU has single hidden state that serves both purposes, which may limit it's capacity  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement chromodb (from versions: none)\n",
      "ERROR: No matching distribution found for chromodb\n"
     ]
    }
   ],
   "source": [
    "# required packages\n",
    "%pip install -q sentence-transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yarae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('mt0rm0/movie_descriptors_small')\n",
    "\n",
    "# print(dataset['train'])\n",
    "# Extract movie titles and synopses\n",
    "movie_titles = [entry['title'] for entry in dataset['train']]\n",
    "movie_synopses = [entry['overview'] for entry in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yarae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the SentenceBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Compute embeddings for all movie synopses\n",
    "synopsis_embeddings = model.encode(movie_synopses, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(query, top_k=5):\n",
    "    # Compute the embedding for the query\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarities between the query embedding and all synopsis embeddings\n",
    "    similarities = util.pytorch_cos_sim(query_embedding, synopsis_embeddings)[0]\n",
    "    \n",
    "    # Get the indices of the top_k most similar synopses\n",
    "    top_k_indices = similarities.topk(k=top_k).indices\n",
    "    \n",
    "    # return the corresponding movie titles\n",
    "    recommended_movies = [movie_titles[idx] for idx in top_k_indices]\n",
    "    return recommended_movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What movie is set in ancient Rome (similar to Gladiator)?\n",
      "1. Agora\n",
      "2. Gladiator\n",
      "3. Cleopatra\n",
      "4. La Dolce Vita\n",
      "5. The Man Who Knew Too Much\n",
      "\n",
      "\n",
      "Query: Please recommend me some movies that are both romantic and funny.\n",
      "1. About Last Night\n",
      "2. Midnight in Paris\n",
      "3. Hysteria\n",
      "4. Closer\n",
      "5. Last Night\n",
      "\n",
      "\n",
      "Query: Which movies are about astronauts and starships?\n",
      "1. Forbidden Planet\n",
      "2. Galaxy Quest\n",
      "3. You Only Live Twice\n",
      "4. Treasure Planet\n",
      "5. The Right Stuff\n",
      "\n",
      "\n",
      "Query: Looking for a thrilling heist movie.\n",
      "1. The Getaway\n",
      "2. The Collector\n",
      "3. Drive\n",
      "4. Baby Driver\n",
      "5. The Score\n",
      "\n",
      "\n",
      "Query: Can you suggest a movie with a strong female lead?\n",
      "1. Swimming Pool\n",
      "2. To Die For\n",
      "3. Thelma & Louise\n",
      "4. The Young Victoria\n",
      "5. Under the Skin\n",
      "\n",
      "\n",
      "Query: I want to watch a movie about artificial intelligence.\n",
      "1. Pi\n",
      "2. Ex Machina\n",
      "3. King Kong\n",
      "4. Ghost in the Shell 2: Innocence\n",
      "5. Gattaca\n",
      "\n",
      "\n",
      "Query: Recommend me some heartwarming family movies.\n",
      "1. Midnight in Paris\n",
      "2. This Is Where I Leave You\n",
      "3. Legends of the Fall\n",
      "4. Scary Movie\n",
      "5. The Proposition\n",
      "\n",
      "\n",
      "Query: What's a good mystery film with lots of twists?\n",
      "1. To Catch a Thief\n",
      "2. Scary Movie\n",
      "3. The Fog\n",
      "4. Margin Call\n",
      "5. Insidious: Chapter 3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example queries\n",
    "queries = [\n",
    "    \"What movie is set in ancient Rome (similar to Gladiator)?\",\n",
    "    \"Please recommend me some movies that are both romantic and funny.\",\n",
    "    \"Which movies are about astronauts and starships?\",\n",
    "    \"Looking for a thrilling heist movie.\",\n",
    "    \"Can you suggest a movie with a strong female lead?\",\n",
    "    \"I want to watch a movie about artificial intelligence.\",\n",
    "    \"Recommend me some heartwarming family movies.\",\n",
    "    \"What's a good mystery film with lots of twists?\"\n",
    "]\n",
    "\n",
    "# Test the system\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    recommendations = recommend_movies(query)\n",
    "    for idx, movie in enumerate(recommendations):\n",
    "        print(f\"{idx+1}. {movie}\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
